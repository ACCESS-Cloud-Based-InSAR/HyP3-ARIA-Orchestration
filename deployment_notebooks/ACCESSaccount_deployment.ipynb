{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This is to demonstrate how to use the `s1-enumerator` to get a full time series of GUNWs.\n",
    "\n",
    "We are going basically take each month in acceptable date range and increment by a month and make sure the temporal window is large enough to ensure connectivity across data gaps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters\n",
    "\n",
    "This is what the operator is going to have to change. Will provide some comments."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# toggle user-controlled parameters here\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# product cutline\n",
    "aoi_shapefile = '../aois/CA_pathNumber115.geojson'\n",
    "# load AO geojson and strip variables\n",
    "with open(aoi_shapefile, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    json_keys = data['features'][0]['properties'].keys()\n",
    "    # assign local variables from dict\n",
    "    locals().update(data['features'][0]['properties'])\n",
    "\n",
    "# Override metadata keys\n",
    "# Warning, toggle with caution\n",
    "# Only to be used if you are testing and intentionally updating AO geojsons\n",
    "update_AO = True\n",
    "\n",
    "### Spatial coverage constraint parameter 'azimuth_mismatch'\n",
    "# The merged SLC area over the AOI is allowed to be smaller by 'azimuth_mismatch' x swath width (i.e. 250km)\n",
    "if 'azimuth_mismatch' not in json_keys or update_AO:\n",
    "    azimuth_mismatch = 5 # adjust as necessary\n",
    "    data['features'][0]['properties']['azimuth_mismatch'] = azimuth_mismatch\n",
    "\n",
    "# Specify deployment URL\n",
    "#deploy_url = 'https://hyp3-tibet-jpl.asf.alaska.edu' #for Tibet\n",
    "deploy_url = 'https://hyp3-nisar-jpl.asf.alaska.edu' #for access\n",
    "data['features'][0]['properties']['deploy_url'] = deploy_url\n",
    "\n",
    "# Number of nearest neighbors\n",
    "if 'num_neighbors' not in json_keys or update_AO:\n",
    "    num_neighbors = 2 # adjust as necessary\n",
    "    data['features'][0]['properties']['num_neighbors'] = num_neighbors\n",
    "\n",
    "#set temporal parameters\n",
    "if 'min_days_backward' not in json_keys or update_AO:\n",
    "    min_days_backward = 0\n",
    "    data['features'][0]['properties']['min_days_backward'] = min_days_backward\n",
    "today = datetime.datetime.now()\n",
    "# Earliest year for reference frames\n",
    "START_YEAR = 2014\n",
    "# Latest year for reference frames\n",
    "END_YEAR = today.year\n",
    "YEARS_OF_INTEREST = list(range(START_YEAR,END_YEAR+1))\n",
    "# Adjust depending on seasonality\n",
    "# For annual IFGs, select a single months of interest and you will get what you want.\n",
    "if 'month_range_lower' not in json_keys or update_AO:\n",
    "    month_range_lower = 1 # adjust as necessary\n",
    "    data['features'][0]['properties']['month_range_lower'] = month_range_lower\n",
    "if 'month_range_upper' not in json_keys or update_AO:\n",
    "    month_range_upper = 12 # adjust as necessary\n",
    "    data['features'][0]['properties']['month_range_upper'] = month_range_upper\n",
    "MONTHS_OF_INTEREST = list(range(month_range_lower,month_range_upper+1))\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "## OPTIONAL set temporal sampling parameters\n",
    "\n",
    "# Temporal sampling invervals\n",
    "if 'min_days_backward_timesubset' not in json_keys or update_AO:\n",
    "    # Specify as many temporal sampling intervals as desired (e.g. 90 (days), 180 (days) = semiannual, 365 (days) = annual, etc.)\n",
    "    min_days_backward_timesubset = []\n",
    "    if min_days_backward_timesubset != []:\n",
    "        data['features'][0]['properties']['min_days_backward_timesubset'] = ','.join(map(str, min_days_backward_timesubset))\n",
    "if 'min_days_backward_timesubset' in json_keys:\n",
    "    min_days_backward_timesubset = [int(s) for s in data['features'][0]['properties']['min_days_backward_timesubset'].split(',')]\n",
    "# apply temporal window to all temporal sampling intervals (hardcoded to 60 days)\n",
    "temporal_window_days_timesubset = 60\n",
    "min_days_backward_timesubset = [i - round(temporal_window_days_timesubset/2) for i in min_days_backward_timesubset]\n",
    "if any(x<1 for x in min_days_backward_timesubset):\n",
    "    raise Exception(\"Your specified 'min_days_backward_timesubset' input is too small relative to\"\n",
    "                    \"your specified 'temporal_window_days_timesubset' value. Adjust accordingly\")\n",
    "\n",
    "# Nearest neighbor sampling\n",
    "if 'num_neighbors_timesubset' not in json_keys or update_AO:\n",
    "    # Specify corresponding nearest neighbor sampling for each temporal sampling interval (by default (n-)1)\n",
    "    num_neighbors_timesubset = []\n",
    "    if num_neighbors_timesubset != []:\n",
    "        data['features'][0]['properties']['num_neighbors_timesubset'] = ','.join(map(str, num_neighbors_timesubset))\n",
    "if 'num_neighbors_timesubset' in json_keys:\n",
    "    num_neighbors_timesubset = [int(s) for s in data['features'][0]['properties']['num_neighbors_timesubset'].split(',')]\n",
    "\n",
    "if len (num_neighbors_timesubset) != len(min_days_backward_timesubset):\n",
    "    raise Exception(\"Specified number of temporal sampling intervals DO NOT match specified nearest neighbor sampling\")\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# Define job-name\n",
    "if 'job_name' not in json_keys or update_AO:\n",
    "    job_name = aoi_shapefile.split('/')[-1].split('.')[0].split('pathNumber')\n",
    "    job_name = ''.join(job_name)\n",
    "    job_name = job_name[-20:]\n",
    "    data['features'][0]['properties']['job_name'] = job_name\n",
    "# product directory\n",
    "prod_dir = job_name\n",
    "\n",
    "# if operator variables do not exist, set and populate geojson with them\n",
    "with open(aoi_shapefile, 'w') as file:\n",
    "    json.dump(data, file)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.729834Z",
     "start_time": "2022-01-26T20:33:20.702152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from s1_enumerator import get_aoi_dataframe,  distill_all_pairs, enumerate_ifgs, get_s1_coverage_tiles, enumerate_ifgs_from_stack, get_s1_stack_by_dataframe\n",
    "import concurrent\n",
    "from rasterio.crs import CRS\n",
    "from s1_enumerator import duplicate_gunw_found\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point, shape\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import networkx as nx\n",
    "import boto3\n",
    "import hyp3_sdk\n",
    "import copy"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.609757Z",
     "start_time": "2022-01-26T20:33:19.458712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def shapefile_area(file_bbox,\n",
    "        bounds = False):\n",
    "    \"\"\"Compute km\\u00b2 area of shapefile.\"\"\"\n",
    "    # import dependencies\n",
    "    from pyproj import Proj\n",
    "\n",
    "    # loop through polygons\n",
    "    shape_area = 0\n",
    "    # pass single polygon as list\n",
    "    if file_bbox.type == 'Polygon': file_bbox = [file_bbox]\n",
    "    for polyobj in file_bbox:\n",
    "        #first check if empty\n",
    "        if polyobj.is_empty:\n",
    "            shape_area += 0\n",
    "            continue\n",
    "        # get coords\n",
    "        if bounds:\n",
    "            # Pass coordinates of bounds as opposed to cutline\n",
    "            # Necessary for estimating DEM/mask footprints\n",
    "            WSEN = polyobj.bounds\n",
    "            lon = np.array([WSEN[0],WSEN[0],WSEN[2],WSEN[2],WSEN[0]])\n",
    "            lat = np.array([WSEN[1],WSEN[3],WSEN[3],WSEN[1],WSEN[1]])\n",
    "        else:\n",
    "            lon, lat = polyobj.exterior.coords.xy\n",
    "\n",
    "        # use equal area projection centered on/bracketing AOI\n",
    "        pa = Proj(\"+proj=aea +lat_1={} +lat_2={} +lat_0={} +lon_0={}\". \\\n",
    "             format(min(lat), max(lat), (max(lat)+min(lat))/2, \\\n",
    "             (max(lon)+min(lon))/2))\n",
    "        x, y = pa(lon, lat)\n",
    "        cop = {\"type\": \"Polygon\", \"coordinates\": [zip(x, y)]}\n",
    "        shape_area += shape(cop).area/1e6  # area in km^2\n",
    "\n",
    "    return shape_area"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.609757Z",
     "start_time": "2022-01-26T20:33:19.458712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def continuous_time(product_df, iter_id='fileID'):\n",
    "    \"\"\"\n",
    "    Split the products into spatiotemporally continuous groups.\n",
    "    Split products by individual, continuous interferograms.\n",
    "    Input must be already sorted by pair and start-time to fit\n",
    "    the logic scheme below.\n",
    "    Using their time-tags, this function determines whether or not\n",
    "    successive products are in the same orbit.\n",
    "    If in the same orbit, the program determines whether or not they\n",
    "    overlap in time and are therefore spatially contiguous,\n",
    "    and rejects/reports cases for which there is no temporal overlap\n",
    "    and therefore a spatial gap.\n",
    "    \"\"\"\n",
    "    from shapely.ops import unary_union\n",
    "\n",
    "    # pass scenes that have no gaps\n",
    "    sorted_products = []\n",
    "    track_rejected_inds = []\n",
    "    pair_dict = {}\n",
    "    product_df_dict = product_df.to_dict('records')\n",
    "    # Check for (and remove) duplicate products\n",
    "    # If multiple pairs in list, cycle through\n",
    "    # and evaluate temporal connectivity.\n",
    "    for i in enumerate(product_df_dict[:-1]):\n",
    "        # Parse the first frame's metadata\n",
    "        scene_start = i[1]['startTime']\n",
    "        scene_end = i[1]['stopTime']\n",
    "        first_frame_ind = i[1]['ind_col']\n",
    "        first_frame = datetime.datetime.strptime( \\\n",
    "                i[1]['fileID'][17:25], \"%Y%m%d\")\n",
    "        # Parse the second frame's metadata\n",
    "        new_scene_start = product_df_dict[i[0]+1]['startTime']\n",
    "        new_scene_end = product_df_dict[i[0]+1]['stopTime']\n",
    "        next_frame_ind = product_df_dict[i[0]+1]['ind_col']\n",
    "        next_frame = datetime.datetime.strptime( \\\n",
    "                product_df_dict[i[0]+1]['fileID'][17:25], \"%Y%m%d\")\n",
    "\n",
    "        # Determine if next product in time is in same orbit AND overlaps\n",
    "        # AND corresponds to same scene\n",
    "        # If it is within same orbit cycle, try to append scene.\n",
    "        # This accounts for day change.\n",
    "        if abs(new_scene_end-scene_end) <= \\\n",
    "                datetime.timedelta(minutes=100) \\\n",
    "                and abs(next_frame-first_frame) <= \\\n",
    "                datetime.timedelta(days=1):\n",
    "            # Don't export product if it is already tracked\n",
    "            # as a rejected scene\n",
    "            if first_frame_ind in track_rejected_inds or \\\n",
    "                    next_frame_ind in track_rejected_inds:\n",
    "                track_rejected_inds.append(first_frame_ind)\n",
    "                track_rejected_inds.append(next_frame_ind)\n",
    "\n",
    "            # Only pass scene if it temporally overlaps with reference scene\n",
    "            elif ((scene_end <= new_scene_start) and \\\n",
    "                    (new_scene_end <= scene_start)) or \\\n",
    "                    ((scene_end >= new_scene_start) and \\\n",
    "                    (new_scene_end >= scene_start)):\n",
    "                # Check if dictionary for scene already exists,\n",
    "                # and if it does then append values\n",
    "                try:\n",
    "                    dict_ind = sorted_products.index(next(item for item \\\n",
    "                            in sorted_products if i[1][iter_id] \\\n",
    "                            in item[iter_id]))\n",
    "                    sorted_products[dict_ind] = {key: np.hstack([value] + \\\n",
    "                         [product_df_dict[i[0]+1][key]]).tolist() \\\n",
    "                         for key, value in sorted_products[dict_ind].items()}\n",
    "                # Match corresponding to scene NOT found,\n",
    "                # so initialize dictionary for new scene\n",
    "                except:\n",
    "                    sorted_products.extend([dict(zip(i[1].keys(), \\\n",
    "                            [list(a) for a in zip(i[1].values(), \\\n",
    "                            product_df_dict[i[0]+1].values())]))])\n",
    "\n",
    "            # Else if scene doesn't overlap, this means there is a gap.\n",
    "            # Reject date from product list,\n",
    "            # and keep track of all failed dates\n",
    "            else:\n",
    "                track_rejected_inds.append(first_frame_ind)\n",
    "                track_rejected_inds.append(next_frame_ind)\n",
    "        # Products correspond to different dates,\n",
    "        # So pass both as separate scenes.\n",
    "        else:\n",
    "            # Check if dictionary for corresponding scene already exists.\n",
    "            if [item for item in sorted_products if i[1][iter_id] in \\\n",
    "                    item[iter_id]]==[] and i[1]['ind_col'] not in \\\n",
    "                    track_rejected_inds:\n",
    "                sorted_products.extend([dict(zip(i[1].keys(), \\\n",
    "                        [list(a) for a in zip(i[1].values())]))])\n",
    "            # Initiate new scene\n",
    "            if [item for item in sorted_products if \\\n",
    "                    product_df_dict[i[0]+1][iter_id] in item[iter_id]]==[] \\\n",
    "                    and next_frame_ind not in track_rejected_inds:\n",
    "                sorted_products.extend([dict(zip( \\\n",
    "                        product_df_dict[i[0]+1].keys(), \\\n",
    "                        [list(a) for a in \\\n",
    "                        zip(product_df_dict[i[0]+1].values())]))])\n",
    "            if first_frame_ind in track_rejected_inds:\n",
    "                track_rejected_inds.append(first_frame_ind)\n",
    "            if next_frame_ind in track_rejected_inds:\n",
    "                track_rejected_inds.append(next_frame_ind)\n",
    "\n",
    "    # Remove duplicate dates\n",
    "    track_rejected_inds = list(set(track_rejected_inds))\n",
    "    if len(track_rejected_inds) > 0:\n",
    "        print(\"{}/{} scenes rejected as stitched IFGs have gaps\".format( \\\n",
    "             len(track_rejected_inds), len(product_df)))\n",
    "        # Provide report of which files were kept vs. which were not.\n",
    "        print(\"Specifically, the following scenes were rejected:\")\n",
    "        for item in product_df_dict:\n",
    "            if item['ind_col'] in track_rejected_inds:\n",
    "                print(item['fileID'])\n",
    "    else:\n",
    "        print(\"All {} scenes are spatially continuous.\".format( \\\n",
    "             len(sorted_products)))\n",
    "\n",
    "    # pass scenes that have no gaps\n",
    "    sorted_products = [item for item in sorted_products \\\n",
    "            if not (any(x in track_rejected_inds for x in item['ind_col']))]\n",
    "\n",
    "    # Report dictionaries for all valid products\n",
    "    if sorted_products == []: #Check if pairs were successfully selected\n",
    "        raise Exception('No scenes meet spatial criteria '\n",
    "                        'due to gaps and/or invalid input. '\n",
    "                        'Nothing to export.')\n",
    "\n",
    "    # Combine polygons\n",
    "    for i in enumerate(sorted_products):\n",
    "        sorted_products[i[0]]['geometry'] = unary_union(i[1]['geometry'])\n",
    "\n",
    "    # combine and record scenes with gaps\n",
    "    track_kept_inds = pd.DataFrame(sorted_products)['ind_col'].to_list()\n",
    "    track_kept_inds = [item for sublist in track_kept_inds for item in sublist]\n",
    "    temp_gap_scenes_dict = [item for item in product_df_dict \\\n",
    "            if not item['ind_col'] in track_kept_inds]\n",
    "    gap_scenes_dict = []\n",
    "    for i in enumerate(temp_gap_scenes_dict[:-1]):\n",
    "        # Parse the first frame's metadata\n",
    "        first_frame_ind = i[1]['ind_col']\n",
    "        first_frame = datetime.datetime.strptime( \\\n",
    "                i[1]['fileID'][17:25], \"%Y%m%d\")\n",
    "        # Parse the second frame's metadata\n",
    "        next_frame_ind = temp_gap_scenes_dict[i[0]+1]['ind_col']\n",
    "        next_frame = datetime.datetime.strptime( \\\n",
    "                temp_gap_scenes_dict[i[0]+1]['fileID'][17:25], \"%Y%m%d\")\n",
    "        # Determine if next product in time is in same orbit\n",
    "        # If it is within same orbit cycle, try to append scene.\n",
    "        # This accounts for day change.\n",
    "        if abs(next_frame-first_frame) <= \\\n",
    "            datetime.timedelta(days=1):\n",
    "            # Check if dictionary for scene already exists,\n",
    "            # and if it does then append values\n",
    "            try:\n",
    "                dict_ind = gap_scenes_dict.index(next(item for item \\\n",
    "                        in gap_scenes_dict if i[1][iter_id] \\\n",
    "                        in item[iter_id]))\n",
    "                gap_scenes_dict[dict_ind] = {key: np.hstack([value] + \\\n",
    "                        [temp_gap_scenes_dict[i[0]+1][key]]).tolist() \\\n",
    "                        for key, value in gap_scenes_dict[dict_ind].items()}\n",
    "            # Match corresponding to scene NOT found,\n",
    "            # so initialize dictionary for new scene\n",
    "            except:\n",
    "                gap_scenes_dict.extend([dict(zip(i[1].keys(), \\\n",
    "                        [list(a) for a in zip(i[1].values(), \\\n",
    "                        temp_gap_scenes_dict[i[0]+1].values())]))])\n",
    "        # Products correspond to different dates,\n",
    "        # So pass both as separate scenes.\n",
    "        else:\n",
    "            # Check if dictionary for corresponding scene already exists.\n",
    "            if [item for item in gap_scenes_dict if i[1][iter_id] in \\\n",
    "                    item[iter_id]]==[]:\n",
    "                gap_scenes_dict.extend([dict(zip(i[1].keys(), \\\n",
    "                        [list(a) for a in zip(i[1].values())]))])\n",
    "            # Initiate new scene\n",
    "            if [item for item in gap_scenes_dict if \\\n",
    "                    temp_gap_scenes_dict[i[0]+1][iter_id] in item[iter_id]]==[]:\n",
    "                gap_scenes_dict.extend([dict(zip( \\\n",
    "                        temp_gap_scenes_dict[i[0]+1].keys(), \\\n",
    "                        [list(a) for a in \\\n",
    "                        zip(temp_gap_scenes_dict[i[0]+1].values())]))])\n",
    "\n",
    "    # there may be some extra missed pairs with gaps\n",
    "    if gap_scenes_dict != []:\n",
    "        extra_track_rejected_inds = pd.DataFrame(gap_scenes_dict)['ind_col'].to_list()\n",
    "        extra_track_rejected_inds = [item for sublist in extra_track_rejected_inds for item in sublist]\n",
    "        track_rejected_inds.extend(extra_track_rejected_inds)\n",
    "\n",
    "    return sorted_products, track_rejected_inds, gap_scenes_dict"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.609757Z",
     "start_time": "2022-01-26T20:33:19.458712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def minimum_overlap_query(tiles,\n",
    "        aoi,\n",
    "        azimuth_mismatch=0.01,\n",
    "        iter_id='fileID'):\n",
    "    \"\"\"\n",
    "    Master function managing checks for SAR scene spatiotemporal contiguity\n",
    "    and filtering out scenes based off of user-defined spatial coverage threshold\n",
    "    \"\"\"\n",
    "    # initiate dataframe\n",
    "    tiles = tiles.sort_values(['startTime'])\n",
    "    updated_tiles = tiles.copy()\n",
    "\n",
    "    # Drop scenes that don't intersect with AOI at all\n",
    "    orig_len = updated_tiles.shape[0]\n",
    "    for index, row in tiles.iterrows():\n",
    "        intersection_area = aoi.intersection(row['geometry'])\n",
    "        overlap_area = shapefile_area(intersection_area)\n",
    "        aoi_area = shapefile_area(aoi)\n",
    "        percentage_coverage = (overlap_area/aoi_area)*100\n",
    "        if percentage_coverage == 0:\n",
    "            drop_ind = updated_tiles[updated_tiles['fileID'] == row['fileID']].index\n",
    "            updated_tiles = updated_tiles.drop(index=drop_ind)\n",
    "    updated_tiles = updated_tiles.reset_index(drop=True)\n",
    "    print(\"{}/{} scenes rejected for not intersecting with the AOI\".format( \\\n",
    "          orig_len-updated_tiles.shape[0], orig_len))\n",
    "\n",
    "    # group IFGs spatiotemporally\n",
    "    updated_tiles['ind_col'] = range(0, len(updated_tiles))\n",
    "    updated_tiles_dict, dropped_indices, gap_scenes_dict = continuous_time(updated_tiles, iter_id)\n",
    "    for i in dropped_indices:\n",
    "        drop_ind = updated_tiles.index[updated_tiles['ind_col'] == i]\n",
    "        updated_tiles.drop(drop_ind, inplace=True)\n",
    "    updated_tiles = updated_tiles.reset_index(drop=True)\n",
    "    \n",
    "    # Kick out scenes that do not meet user-defined spatial threshold\n",
    "    aoi_area = shapefile_area(aoi)\n",
    "    orig_len = updated_tiles.shape[0]\n",
    "    track_rejected_inds = []\n",
    "    minimum_overlap_threshold = aoi_area - (250 * azimuth_mismatch)\n",
    "    print(\"\")\n",
    "    print(\"AOI coverage: {}\".format(aoi_area))\n",
    "    print(\"Allowable area of miscoverage: {}\".format(250 * azimuth_mismatch))\n",
    "    print(\"minimum_overlap_threshold: {}\".format(minimum_overlap_threshold))\n",
    "    print(\"\")\n",
    "    if minimum_overlap_threshold < 0:\n",
    "        raise Exception('WARNING: user-defined mismatch of {}km\\u00b2 too large relative to specified AOI'.format(azimuth_mismatch))\n",
    "    for i in enumerate(updated_tiles_dict):\n",
    "        intersection_area = aoi.intersection(i[1]['geometry'])\n",
    "        overlap_area = shapefile_area(intersection_area)\n",
    "        # Kick out scenes below specified overlap threshold\n",
    "        if minimum_overlap_threshold > overlap_area:\n",
    "            for iter_ind in enumerate(i[1]['ind_col']):\n",
    "                track_rejected_inds.append(iter_ind[1])\n",
    "                print(\"Rejected scene {} has only {}km\\u00b2 overlap with AOI\".format( \\\n",
    "                    i[1]['fileID'][iter_ind[0]], int(overlap_area)))\n",
    "                drop_ind = updated_tiles[updated_tiles['ind_col'] == iter_ind[1]].index\n",
    "                updated_tiles = updated_tiles.drop(index=drop_ind)\n",
    "    updated_tiles = updated_tiles.reset_index(drop=True)\n",
    "    print(\"{}/{} scenes rejected for not meeting defined spatial criteria\".format( \\\n",
    "          orig_len-updated_tiles.shape[0], orig_len))\n",
    "\n",
    "    # record rejected scenes separately\n",
    "    rejected_scenes_dict = [item for item in updated_tiles_dict \\\n",
    "            if (any(x in track_rejected_inds for x in item['ind_col']))]\n",
    "    # pass scenes that are not tracked as rejected\n",
    "    updated_tiles_dict = [item for item in updated_tiles_dict \\\n",
    "            if not (any(x in track_rejected_inds for x in item['ind_col']))]\n",
    "\n",
    "    return updated_tiles, pd.DataFrame(updated_tiles_dict), pd.DataFrame(gap_scenes_dict), pd.DataFrame(rejected_scenes_dict)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.609757Z",
     "start_time": "2022-01-26T20:33:19.458712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pair_spatial_check(tiles,\n",
    "        aoi,\n",
    "        azimuth_mismatch=0.01,\n",
    "        iter_id='fileID'):\n",
    "    \"\"\"\n",
    "    Santity check function to confirm selected pairs meet user-defined spatial coverage threshold\n",
    "    \"\"\"\n",
    "    tiles['ind_col'] = range(0, len(tiles))\n",
    "    tiles = tiles.drop(columns=['reference', 'secondary'])\n",
    "    tiles_dict, dropped_pairs, gap_scenes_dict = continuous_time(tiles, iter_id='ind_col')\n",
    "\n",
    "    # Kick out scenes that do not meet user-defined spatial threshold\n",
    "    aoi_area = shapefile_area(aoi)\n",
    "    orig_len = tiles.shape[0]\n",
    "    track_rejected_inds = []\n",
    "    minimum_overlap_threshold = aoi_area - (250 * azimuth_mismatch)\n",
    "    if minimum_overlap_threshold < 0:\n",
    "        raise Exception('WARNING: user-defined mismatch of {}km\\u00b2 too large relative to specified AOI'.format(azimuth_mismatch))\n",
    "    for i in enumerate(tiles_dict):\n",
    "        intersection_area = aoi.intersection(i[1]['geometry'])\n",
    "        overlap_area = shapefile_area(intersection_area)\n",
    "        # Kick out scenes below specified overlap threshold\n",
    "        if minimum_overlap_threshold > overlap_area:\n",
    "            for iter_ind in enumerate(i[1]['ind_col']):\n",
    "                track_rejected_inds.append(iter_ind[1])\n",
    "                print(\"Rejected pair {} has only {}km\\u00b2 overlap with AOI {}ID {}Ind\".format( \\\n",
    "                      i[1]['reference_date'][iter_ind[0]].replace('-', '') + '_' + \\\n",
    "                      i[1]['secondary_date'][iter_ind[0]].replace('-', ''), \\\n",
    "                      overlap_area, iter_ind[1], i[0]))\n",
    "                drop_ind = tiles[tiles['ind_col'] == iter_ind[1]].index\n",
    "                tiles = tiles.drop(index=drop_ind)\n",
    "    tiles = tiles.reset_index(drop=True)\n",
    "    print(\"{}/{} scenes rejected for not meeting defined spatial criteria\".format( \\\n",
    "          orig_len-tiles.shape[0], orig_len))\n",
    "\n",
    "    # record rejected scenes separately\n",
    "    rejected_scenes_dict = [item for item in tiles_dict \\\n",
    "            if (any(x in track_rejected_inds for x in item['ind_col']))]\n",
    "    # pass scenes that are not tracked as rejected\n",
    "    tiles_dict = [item for item in tiles_dict \\\n",
    "            if not (any(x in track_rejected_inds for x in item['ind_col']))]\n",
    "\n",
    "    return pd.DataFrame(tiles_dict), pd.DataFrame(gap_scenes_dict), pd.DataFrame(rejected_scenes_dict)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.609757Z",
     "start_time": "2022-01-26T20:33:19.458712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_aoi = gpd.read_file(aoi_shapefile)\n",
    "aoi = df_aoi.geometry.unary_union\n",
    "aoi"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.662349Z",
     "start_time": "2022-01-26T20:33:20.614595Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Currently, there is a lot of data in each of the rows above. We really only need the AOI `geometry` and the `path_number`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_numbers = df_aoi.path_number.unique().tolist()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.871551Z",
     "start_time": "2022-01-26T20:33:20.834020Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate a stack\n",
    "\n",
    "Using all the tiles that are needed to cover the AOI we make a geometric query based on the frame. We now include only the path we are interested in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_dict = {}\n",
    "path_dict['pathNumber'] = str(path_numbers[0])\n",
    "aoi_geometry = pd.DataFrame([path_dict])\n",
    "aoi_geometry = gpd.GeoDataFrame(aoi_geometry, geometry=[shape(aoi)], crs=CRS.from_epsg(4326))\n",
    "aoi_geometry['pathNumber'] = aoi_geometry['pathNumber'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_stack = get_s1_stack_by_dataframe(aoi_geometry,\n",
    "                                     path_numbers=path_numbers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f'We have {df_stack.shape[0]} frames in our stack'"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.080329Z",
     "start_time": "2022-01-26T20:33:26.048785Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "df_stack.plot(ax=ax, alpha=.5, color='green', label='Frames interesecting tile')\n",
    "df_aoi.exterior.plot(color='black', ax=ax, label='AOI')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.322384Z",
     "start_time": "2022-01-26T20:33:26.081992Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note, we now see the frames cover the entire AOI as we expect."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First remove all scenes that do not produce spatiotemporally contiguous pairs and not meet specified intersection threshold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_stack, df_stack_dict, gap_scenes_dict, rejected_scenes_dict = minimum_overlap_query(df_stack, aoi, azimuth_mismatch=azimuth_mismatch)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.080329Z",
     "start_time": "2022-01-26T20:33:26.048785Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f'We have {df_stack.shape[0]} frames in our stack'"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.080329Z",
     "start_time": "2022-01-26T20:33:26.048785Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot acquisitions that aren't continuous (i.e. have gaps)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not gap_scenes_dict.empty:\n",
    "    gap_scenes_dict =  gap_scenes_dict.sort_values(by=['start_date'])\n",
    "    for index, row in gap_scenes_dict.iterrows():\n",
    "        fig, ax = plt.subplots()\n",
    "        p = gpd.GeoSeries(row['geometry'])\n",
    "        p.exterior.plot(color='black', ax=ax, label=row['start_date_str'][0])\n",
    "        df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "        plt.legend()\n",
    "        plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot all mosaicked acquisitions that were rejected for not meeting user-specified spatial constraints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not rejected_scenes_dict.empty:\n",
    "    rejected_scenes_dict =  rejected_scenes_dict.sort_values(by=['start_date'])\n",
    "    fig, ax = plt.subplots()\n",
    "    for index, row in rejected_scenes_dict.iterrows():\n",
    "        p = gpd.GeoSeries(row['geometry'])\n",
    "        p.exterior.plot(color='black', ax=ax)\n",
    "    df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "    plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot each individual mosaicked acquisitions that were rejected for not meeting user-specified spatial constraints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not rejected_scenes_dict.empty:\n",
    "    for index, row in rejected_scenes_dict.iterrows():\n",
    "        fig, ax = plt.subplots()\n",
    "        p = gpd.GeoSeries(row['geometry'])\n",
    "        p.exterior.plot(color='black', ax=ax, label=row['start_date_str'][0])\n",
    "        df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "        plt.legend()\n",
    "        plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot all mosaicked acquisitions that meet user-defined spatial coverage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for index, row in df_stack_dict.iterrows():\n",
    "    p = gpd.GeoSeries(row['geometry'])\n",
    "    p.exterior.plot(color='black', ax=ax)\n",
    "df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we filter the stack by month to ensure we only have SLCs we need."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_stack_month = df_stack[df_stack.start_date.dt.month.isin(MONTHS_OF_INTEREST)]\n",
    "df_stack_month = df_stack_month[df_stack_month.start_date.dt.year.isin(YEARS_OF_INTEREST)]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.356073Z",
     "start_time": "2022-01-26T20:33:26.324043Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will create a list of ```min_reference_dates``` in descending order starting with the most recent date from the SLC stack ```df_stack_month``` as the start date."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_reference_dates = sorted(df_stack_month['startTime'].to_list())\n",
    "min_reference_dates = sorted(list(set([i.replace(hour=0, minute=0, second=0) for i in min_reference_dates])), reverse = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now enumerate the SLC pairs that will produce the interferograms (GUNWs) based on initially defined parameters that are exposed at the top-level of this jupyter notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ifg_pairs = []\n",
    "temporal_window_days = 365*3\n",
    "\n",
    "# Avoid duplicate reference scenes (i.e. extra neighbors than intended)\n",
    "track_ref_dates = []\n",
    "for min_ref_date in tqdm(min_reference_dates):\n",
    "    temp = enumerate_ifgs_from_stack(df_stack_month,\n",
    "                                     aoi,\n",
    "                                     min_ref_date,\n",
    "                                     enumeration_type='tile', # options are 'tile' and 'path'. 'path' processes multiple references simultaneously\n",
    "                                     min_days_backward=min_days_backward,\n",
    "                                     num_neighbors_ref=1,\n",
    "                                     num_neighbors_sec=num_neighbors,\n",
    "                                     temporal_window_days=temporal_window_days,\n",
    "                                     min_tile_aoi_overlap_km2=.1,#Minimum reference tile overlap of AOI in km2\n",
    "                                     min_ref_tile_overlap_perc=.1,#Relative overlap of secondary frames over reference frame\n",
    "                                     minimum_ifg_area_km2=0.1,#The minimum overlap of reference and secondary in km2\n",
    "                                     minimum_path_intersection_km2=.1,#Overlap of common track union with respect to AOI in km2\n",
    "                                         )\n",
    "    if temp != []:\n",
    "        iter_key = temp[0]['reference']['start_date'].keys()[0]\n",
    "        iter_references_scenes = [temp[0]['reference']['start_date'][iter_key]]\n",
    "        if not any(x in iter_references_scenes for x in track_ref_dates):\n",
    "            track_ref_dates.extend(iter_references_scenes)            \n",
    "            ifg_pairs += temp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "OPTIONAL densify network with temporal sampling parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Densify network with specified temporal sampling\n",
    "\n",
    "# Avoid duplicate reference scenes (i.e. extra neighbors than intended)\n",
    "if min_days_backward_timesubset != []:\n",
    "    for t_ind,t_interval in enumerate(min_days_backward_timesubset):\n",
    "        track_ref_dates = []\n",
    "        for min_ref_date in tqdm(min_reference_dates):\n",
    "            temp = enumerate_ifgs_from_stack(df_stack_month,\n",
    "                                             aoi,\n",
    "                                             min_ref_date,\n",
    "                                             enumeration_type='tile', # options are 'tile' and 'path'. 'path' processes multiple references simultaneously\n",
    "                                             min_days_backward=t_interval,\n",
    "                                             num_neighbors_ref=1,\n",
    "                                             num_neighbors_sec=num_neighbors_timesubset[t_ind],\n",
    "                                             temporal_window_days=temporal_window_days_timesubset,\n",
    "                                             min_tile_aoi_overlap_km2=.1,#Minimum reference tile overlap of AOI in km2\n",
    "                                             min_ref_tile_overlap_perc=.1,#Relative overlap of secondary frames over reference frame\n",
    "                                             minimum_ifg_area_km2=0.1,#The minimum overlap of reference and secondary in km2\n",
    "                                             minimum_path_intersection_km2=.1,#Overlap of common track union with respect to AOI in km2\n",
    "                                                 )\n",
    "            if temp != []:\n",
    "                iter_key = temp[0]['reference']['start_date'].keys()[0]\n",
    "                iter_references_scenes = [temp[0]['reference']['start_date'][iter_key]]\n",
    "                if not any(x in iter_references_scenes for x in track_ref_dates):\n",
    "                    track_ref_dates.extend(iter_references_scenes)            \n",
    "                    ifg_pairs += temp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f'The number of GUNWs (likely lots of duplicates) is {len(ifg_pairs)}'"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:46.839854Z",
     "start_time": "2022-01-26T20:34:46.810852Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pairs = distill_all_pairs(ifg_pairs)\n",
    "f\"# of GUNWs: ' {df_pairs.shape[0]}\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:47.146266Z",
     "start_time": "2022-01-26T20:34:47.117927Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a sanity check, confirm all IFG pairs meet user-defined spatial coverage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if there are any gaps in the mosaicked IFGs, or if any are rejected for not meeting user-specified spatial constraints\n",
    "\n",
    "*NOTE: No products should be rejected at this stage. If any are, there is a problem either due to a:\n",
    "1) Loose constraint on `azimuth_mismatch` variable whereby scenes not encompassing the entire AOI are getting passed\n",
    "2) User-driven error\n",
    "3) Bug in the code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pairs_dict, df_pairs_gap_scenes_dict, df_pairs_rejected_scenes_dict = pair_spatial_check(df_pairs, aoi, azimuth_mismatch=azimuth_mismatch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot acquisitions that aren't continuous (i.e. have gaps)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not df_pairs_gap_scenes_dict.empty:\n",
    "    for index, row in df_pairs_gap_scenes_dict.iterrows():\n",
    "        fig, ax = plt.subplots()\n",
    "        p = gpd.GeoSeries(row['geometry'])\n",
    "        p.exterior.plot(color='black', ax=ax, label=row['reference_date'][0] + '_' + row['secondary_date'][0])\n",
    "        df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "        plt.legend()\n",
    "        plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot all mosaicked acquisitions that were rejected for not meeting user-specified spatial constraints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not df_pairs_rejected_scenes_dict.empty:\n",
    "    fig, ax = plt.subplots()\n",
    "    for index, row in df_pairs_rejected_scenes_dict.iterrows():\n",
    "        p = gpd.GeoSeries(row['geometry'])\n",
    "        p.exterior.plot(color='black', ax=ax)\n",
    "    df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "    plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot each individual mosaicked acquisitions that were rejected for not meeting user-specified spatial constraints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not df_pairs_rejected_scenes_dict.empty:\n",
    "    for index, row in df_pairs_rejected_scenes_dict.iterrows():\n",
    "        fig, ax = plt.subplots()\n",
    "        p = gpd.GeoSeries(row['geometry'])\n",
    "        p.exterior.plot(color='black', ax=ax, label=row['reference_date'][0] + '_' + row['secondary_date'][0])\n",
    "        df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "        plt.legend()\n",
    "        plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot all mosaicked acquisitions that meet user-defined spatial coverage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for index, row in df_pairs_dict.iterrows():\n",
    "    p = gpd.GeoSeries(row['geometry'])\n",
    "    p.exterior.plot(color='black', ax=ax)\n",
    "df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deduplication Pt. 1\n",
    "\n",
    "A `GUNW` is uniquely determined by the reference and secondary IDs. We contanenate these sorted lists and generate a lossy hash to deduplicate products we may have introduced from the enumeration above."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T00:51:53.374968Z",
     "start_time": "2022-01-21T00:51:53.344139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "\n",
    "def get_gunw_hash_id(reference_ids: list, secondary_ids: list) -> str:\n",
    "    all_ids = json.dumps([' '.join(sorted(reference_ids)),\n",
    "                          ' '.join(sorted(secondary_ids))\n",
    "                          ]).encode('utf8')\n",
    "    hash_id = hashlib.md5(all_ids).hexdigest()\n",
    "    return hash_id\n",
    "\n",
    "def hasher(row):\n",
    "    return get_gunw_hash_id(row['reference'], row['secondary'])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:47.176882Z",
     "start_time": "2022-01-26T20:34:47.147987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pairs['hash_id'] = df_pairs.apply(hasher, axis=1)\n",
    "f\"# of duplicated entries: {df_pairs.duplicated(subset=['hash_id']).sum()}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pairs = df_pairs.drop_duplicates(subset=['hash_id']).reset_index(drop=True)\n",
    "f\"# of UNIQUE GUNWs: {df_pairs.shape[0]}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Viewing GUNW pairs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# start index\n",
    "M = 0\n",
    "# number of pairs to view\n",
    "N = 5\n",
    "\n",
    "for J in range(M, M + N):\n",
    "    pair = ifg_pairs[J]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, sharey=True, sharex=True)\n",
    "\n",
    "    df_ref_plot = pair['reference']\n",
    "    df_sec_plot = pair['secondary']\n",
    "\n",
    "    df_ref_plot.plot(column='start_date_str', \n",
    "                     legend=True, \n",
    "                     ax=axs[0], alpha=.15)\n",
    "    df_aoi.exterior.plot(ax=axs[0], alpha=.5, color='black')\n",
    "    axs[0].set_title('Reference')\n",
    "\n",
    "    df_sec_plot.plot(column='start_date_str', \n",
    "                     legend=True, \n",
    "                     ax=axs[1], alpha=.15)\n",
    "    df_aoi.exterior.plot(ax=axs[1], alpha=.5, color='black')\n",
    "    \n",
    "    axs[0].set_title(f'Reference {J}')\n",
    "    axs[1].set_title('Secondary')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:48.692684Z",
     "start_time": "2022-01-26T20:34:47.290616Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update types for Graphical Analysis\n",
    "\n",
    "We want to do some basic visualization to support the understanding if we traverse time correctly. We do some simple standard pandas manipulation."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T00:54:06.533180Z",
     "start_time": "2022-01-21T00:54:06.503673Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pairs['reference_date'] = pd.to_datetime(df_pairs['reference_date'])\n",
    "df_pairs['secondary_date'] = pd.to_datetime(df_pairs['secondary_date'])\n",
    "df_pairs.head()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:48.738198Z",
     "start_time": "2022-01-26T20:34:48.698474Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize a Date Graph from Time Series\n",
    "\n",
    "We can put this into a network Directed Graph and use some simple network functions to check connectivity.\n",
    "\n",
    "We are going to use just dates for nodes, though you could use `(ref_date, hash_id)` for nodes and then inspect connected components. That is for another notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get unique dates\n",
    "unique_dates = df_pairs.reference_date.tolist() + df_pairs.secondary_date.tolist()\n",
    "unique_dates = sorted(list(set(unique_dates)))\n",
    "\n",
    "# initiate and plot date notes\n",
    "date2node = {date: k for (k, date) in enumerate(unique_dates)}\n",
    "node2date = {k: date for (date, k) in date2node.items()}\n",
    "\n",
    "%matplotlib widget\n",
    "G = nx.DiGraph()\n",
    "\n",
    "edges = [(date2node[ref_date], date2node[sec_date]) \n",
    "         for (ref_date, sec_date) in zip(df_pairs.reference_date, df_pairs.secondary_date)]\n",
    "G.add_edges_from(edges)\n",
    "nx.draw(G)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function checks there is a path from the first date to the last one. The y-axis is created purely for display so doesn't really indicated anything but flow by month."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nx.has_path(G, \n",
    "            target=date2node[unique_dates[0]],\n",
    "            source=date2node[unique_dates[-1]])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:49.562722Z",
     "start_time": "2022-01-26T20:34:49.532969Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure that the result above returns a ```True``` value to be able to produce a time-series."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "increment = [date.month + date.day for date in unique_dates]\n",
    "\n",
    "# source: https://stackoverflow.com/a/27852570\n",
    "scat = ax.scatter(unique_dates, increment)\n",
    "position = scat.get_offsets().data\n",
    "\n",
    "pos = {date2node[date]: position[k] for (k, date) in enumerate(unique_dates)}\n",
    "nx.draw_networkx_edges(G, pos=pos, ax=ax)\n",
    "ax.grid('on')\n",
    "ax.tick_params(axis='x',\n",
    "               which='major',\n",
    "               labelbottom=True,\n",
    "               labelleft=True)\n",
    "ymin, ymax = ax.get_ylim()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:51.843019Z",
     "start_time": "2022-01-26T20:34:49.564366Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deduplication Pt. 2\n",
    "\n",
    "This is to ensure that previous processing hasn't generate any of the products we have just enumerated.\n",
    "\n",
    "\n",
    "# Check CMR\n",
    "\n",
    "This function checks the ASF DAAC if there are GUNWs with the same spatial extent and same date pairs as the ones created. At some point, we will be able to check the input SLC ids from CMR, but currently that is not possible.\n",
    "\n",
    "If you are processing a new AOI whose products have not been delivered, you can ignore this step. It is a bit time consuming as the queries are done product by product."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import requests\n",
    "\n",
    "COLLECTION_CONCEPT_ID = 'C1595422627-ASF'\n",
    "CMR_URL = 'https://cmr.earthdata.nasa.gov/search/granules.echo10'\n",
    "\n",
    "def parse_echo10(echo10_xml: str):\n",
    "    granules = []\n",
    "    root = ET.fromstring(echo10_xml)\n",
    "    for granule in root.findall('result/Granule'):\n",
    "        g = {\n",
    "            'product_id': granule.find('GranuleUR').text,\n",
    "            'product_version': granule.find('GranuleUR').text.split('-')[-1],\n",
    "            'reference_scenes': [],\n",
    "            'secondary_scenes': []\n",
    "        }\n",
    "        for input_granule in granule.findall('InputGranules/InputGranule'):\n",
    "            input_granule_type, input_granule_name = input_granule.text.split(' ')\n",
    "            if input_granule_type == '[Reference]':\n",
    "                g['reference_scenes'].append(input_granule_name)\n",
    "            else:\n",
    "                g['secondary_scenes'].append(input_granule_name)\n",
    "        granules.append(g)\n",
    "    return granules\n",
    "\n",
    "\n",
    "def get_cmr_products(path: int = None):\n",
    "    session = requests.Session()\n",
    "    search_params = {\n",
    "        'provider': 'ASF',\n",
    "        'collection_concept_id': COLLECTION_CONCEPT_ID,\n",
    "        'page_size': 2000,\n",
    "    }\n",
    "    if path is not None:\n",
    "        search_params['attribute[]'] = f'int,PATH_NUMBER,{path}'\n",
    "    headers = {}\n",
    "    products = []\n",
    "\n",
    "    while True:\n",
    "        response = session.get(CMR_URL, params=search_params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        parsed_results = parse_echo10(response.text)\n",
    "        products.extend(parsed_results)\n",
    "\n",
    "        if 'CMR-Search-After' not in response.headers:\n",
    "            break\n",
    "        headers = {'CMR-Search-After': response.headers['CMR-Search-After']}\n",
    "\n",
    "    return products"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# query CMR for all existing products in path\n",
    "results = []\n",
    "results = get_cmr_products(path_numbers[0])\n",
    "\n",
    "# sort with descending product version numbers\n",
    "results = sorted(results, key=lambda d: d['product_version'], reverse = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert CMR results to dataframe with the latest product version\n",
    "new_results = []\n",
    "track_scenes = []\n",
    "for i in results:\n",
    "    ifg_append = i['reference_scenes'] + i['secondary_scenes']\n",
    "    # pass only first instance of scene combo\n",
    "    if ifg_append not in track_scenes:\n",
    "        track_scenes.append(ifg_append)\n",
    "        new_results.append(i)\n",
    "\n",
    "results = pd.DataFrame(new_results)\n",
    "# update column names for merging\n",
    "results.rename(columns={\"reference_scenes\": \"reference\", \"secondary_scenes\": \"secondary\"}, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Capture products in CMR"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def capture_cmr_products(row, cmr_products):\n",
    "    '''Capture products that exist in CMR based on reference and secondary scenes'''\n",
    "    # concatenate reference and secondary scene in each dataframe row\n",
    "    row_scenes = row['reference'] + row['secondary']\n",
    "    # flag True if scenes from enumerator results are within CMR results\n",
    "    if any(set(row_scenes).issubset(set(x)) for x in cmr_products):\n",
    "        product_on_cmr = True\n",
    "    else:\n",
    "        product_on_cmr = np.nan\n",
    "    return product_on_cmr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Finally filter out pairs in CMR\n",
    "try:\n",
    "    # parse all reference and secondary products for each corresponding product in CMR\n",
    "    cmr_products = results['reference'] + results['secondary']\n",
    "    cmr_products = cmr_products.to_list()\n",
    "    # determine which products in the enumerator already exist in CMR\n",
    "    df_pairs['product_id'] = df_pairs.apply(lambda r: capture_cmr_products(r, cmr_products), axis=1)\n",
    "    # filter out pairs in CMR\n",
    "    total_existing_gunws = len(df_pairs[df_pairs['product_id'].notna()])\n",
    "    print('existing_gunws: ', total_existing_gunws)\n",
    "    print('Total pairs', df_pairs.shape[0])\n",
    "    df_pairs_filtered = df_pairs[~df_pairs['product_id'].notna()].reset_index(drop=True)\n",
    "    df_pairs_filtered.drop_duplicates(subset=['hash_id'], inplace=True)\n",
    "    print('after filtering, total pairs: ', df_pairs_filtered.shape[0])\n",
    "except KeyError:\n",
    "    df_pairs_filtered = copy.deepcopy(df_pairs)\n",
    "    df_pairs_filtered.drop_duplicates(subset=['hash_id'], inplace=True)\n",
    "    print('after filtering, total pairs: ', df_pairs_filtered.shape[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after filtering, total pairs:  184\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:13.765308Z",
     "start_time": "2022-01-26T20:35:13.717055Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if len(df_pairs_filtered) == 0:\n",
    "    raise Exception('All queried pairs are in CMR, there is nothing to process with specified parameters.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check Hyp3 Account\n",
    "\n",
    "We are now going to check\n",
    "\n",
    "1. check products in the open s3 bucket\n",
    "2. check running/pending jobs\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. Above, to accomplish step 1., there is some verbose code (see below). Once we automate delivery, this step will be obsolete. However, until we have delivery, we have to make sure that there are no existing products. Additionally, if we are using a separate (non-operational account), then would be good to use this.\n",
    "2. If we are debugging products and some of our previously generated products were made incorrectly, we will want to ignore this step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# uses .netrc; add `prompt=True` to prompt for credentials; \n",
    "hyp3_isce = hyp3_sdk.HyP3(deploy_url)\n",
    "pending_jobs = hyp3_isce.find_jobs(status_code='PENDING') +  hyp3_isce.find_jobs(status_code='RUNNING')\n",
    "all_jobs = hyp3_isce.find_jobs()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:22.029890Z",
     "start_time": "2022-01-26T20:35:13.809788Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(all_jobs)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:22.059552Z",
     "start_time": "2022-01-26T20:35:22.031459Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Get existing products in s3 bucket"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get bucket (there is only one)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_data = [j.to_dict() for j in all_jobs]\n",
    "job_data_s3 = list(filter(lambda job: 'files' in job.keys(), job_data))\n",
    "bucket = job_data_s3[0]['files'][0]['s3']['bucket']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get all keys"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_keys = [job['files'][0]['s3']['key'] for job in job_data_s3]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:22.187071Z",
     "start_time": "2022-01-26T20:35:22.156852Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "s3 = boto3.resource('s3',config=Config(signature_version=UNSIGNED))\n",
    "prod_bucket = s3.Bucket(bucket)\n",
    "\n",
    "objects = list(prod_bucket.objects.all())\n",
    "ncs = list(filter(lambda x: x.key.endswith('.nc'), objects))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.044608Z",
     "start_time": "2022-01-26T20:35:22.188601Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Need to physically check if the products are not there (could have been deleted!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nc_keys = [nc_ob.key for nc_ob in ncs]\n",
    "jobs_with_prods_in_s3 = [job for (k, job) in enumerate(job_data_s3) if job_keys[k] in nc_keys]\n",
    "\n",
    "slcs = [(job['job_parameters']['granules'],\n",
    "         job['job_parameters']['secondary_granules']) \n",
    "        for job in jobs_with_prods_in_s3]\n",
    "\n",
    "hash_ids_of_prods_in_s3 = [get_gunw_hash_id(*slc) for slc in slcs]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.077867Z",
     "start_time": "2022-01-26T20:35:27.046163Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f\"We are removing {df_pairs_filtered['hash_id'].isin(hash_ids_of_prods_in_s3).sum()} GUNWs for submission\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.174758Z",
     "start_time": "2022-01-26T20:35:27.143742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "items = hash_ids_of_prods_in_s3\n",
    "df_pairs_filtered = df_pairs_filtered[~df_pairs_filtered['hash_id'].isin(items)].reset_index(drop=True)\n",
    "f\"Current # of GUNWs: {df_pairs_filtered.shape[0]}\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.207821Z",
     "start_time": "2022-01-26T20:35:27.176177Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Running or Pending Jobs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pending_job_data = [j.to_dict() for j in pending_jobs]\n",
    "pending_slcs = [(job['job_parameters']['granules'],\n",
    "                 job['job_parameters']['secondary_granules']) \n",
    "                 for job in pending_job_data]\n",
    "\n",
    "hash_ids_of_pending_jobs = [get_gunw_hash_id(*slc) for slc in pending_slcs]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.237660Z",
     "start_time": "2022-01-26T20:35:27.209164Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "items = hash_ids_of_pending_jobs\n",
    "f\"We are removing {df_pairs_filtered['hash_id'].isin(items).sum()} GUNWs for submission\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.299781Z",
     "start_time": "2022-01-26T20:35:27.270417Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "items = hash_ids_of_pending_jobs\n",
    "df_pairs_filtered = df_pairs_filtered[~df_pairs_filtered['hash_id'].isin(items)].reset_index(drop=True)\n",
    "f\"Current # of GUNWs: {df_pairs_filtered.shape[0]}\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.333433Z",
     "start_time": "2022-01-26T20:35:27.301301Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize a Date Graph from the Final Filtered Time Series\n",
    "\n",
    "We can put this into a network Directed Graph and use some simple network functions to check connectivity (*which may not be applicable).\n",
    "\n",
    "We are going to use just dates for nodes, though you could use `(ref_date, hash_id)` for nodes and then inspect connected components. That is for another notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get unique dates\n",
    "unique_dates = df_pairs_filtered.reference_date.tolist() + df_pairs_filtered.secondary_date.tolist()\n",
    "unique_dates = sorted(list(set(unique_dates)))\n",
    "\n",
    "# initiate and plot date notes\n",
    "date2node = {date: k for (k, date) in enumerate(unique_dates)}\n",
    "node2date = {k: date for (date, k) in date2node.items()}\n",
    "\n",
    "%matplotlib widget\n",
    "G = nx.DiGraph()\n",
    "\n",
    "edges = [(date2node[ref_date], date2node[sec_date]) \n",
    "         for (ref_date, sec_date) in zip(df_pairs_filtered.reference_date, df_pairs_filtered.secondary_date)]\n",
    "G.add_edges_from(edges)\n",
    "nx.draw(G)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function checks there is a path from the first date to the last one. The y-axis is created purely for display so doesn't really indicated anything but flow by month."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Again, this may not be applicable in cases where parts of the network had already been deployed before and/or you are densifying by specifying temporal sampling.\n",
    "In such cases, these plots serve merely as a sanity check."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nx.has_path(G, \n",
    "            target=date2node[unique_dates[0]],\n",
    "            source=date2node[unique_dates[-1]])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:49.562722Z",
     "start_time": "2022-01-26T20:34:49.532969Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure that the result above returns a ```True``` value to be able to produce a time-series."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "increment = [date.month + date.day for date in unique_dates]\n",
    "\n",
    "# source: https://stackoverflow.com/a/27852570\n",
    "scat = ax.scatter(unique_dates, increment)\n",
    "position = scat.get_offsets().data\n",
    "\n",
    "pos = {date2node[date]: position[k] for (k, date) in enumerate(unique_dates)}\n",
    "nx.draw_networkx_edges(G, pos=pos, ax=ax)\n",
    "ax.grid('on')\n",
    "ax.tick_params(axis='x',\n",
    "               which='major',\n",
    "               labelbottom=True,\n",
    "               labelleft=True)\n",
    "ymin, ymax = ax.get_ylim()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:51.843019Z",
     "start_time": "2022-01-26T20:34:49.564366Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submit jobs to Hyp3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "records_to_submit = df_pairs_filtered.to_dict('records')\n",
    "records_to_submit[0]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.366597Z",
     "start_time": "2022-01-26T20:35:27.335100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The below puts the records in a format that we can submit to the Hyp3 API.\n",
    "\n",
    "**Note 1**: there is an index in the records to submit to ensure we don't over submit jobs for generating GUNWs. \\\n",
    "**Note 2**: uncomment the code to *actually* submit the jobs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# uses .netrc; add `prompt=True` to prompt for credentials; \n",
    "hyp3_isce = hyp3_sdk.HyP3(deploy_url)\n",
    "\n",
    "# NOTE: we are using \"INSAR_ISCE\" for the `main` branch.\n",
    "# Change this to \"INSAR_ISCE_TEST\" to use the `dev` branch, but ONLY if you know what you're doing\n",
    "# chaging to dev will overwrite the product version number and make it difficult to dedup\n",
    "job_type = 'INSAR_ISCE'\n",
    "\n",
    "job_dicts = [{'name': job_name,\n",
    "              'job_type': job_type,\n",
    "              'job_parameters': {'granules': r['reference'],\n",
    "                                 'secondary_granules': r['secondary']}} \n",
    "             # NOTE THERE IS AN INDEX - this is to submit only a subset of Jobs\n",
    "             for r in records_to_submit]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:33.012813Z",
     "start_time": "2022-01-26T20:35:30.285294Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Report summary of all job parameters\n",
    "print(\"Start date is '{}'\".format(unique_dates[0]))\n",
    "print(\"End date is '{}'\".format(unique_dates[-1]))\n",
    "print(\"GUNWs expected '{}'\".format(len(job_dicts)))\n",
    "print(\"Job Name is '{}'\".format(job_name))\n",
    "print(\"Shapefile Name is '{}'\".format(aoi_shapefile.split('/')[-1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#UNCOMMENT TO SUBMIT\n",
    "#prepared_jobs = job_dicts\n",
    "#submitted_jobs = hyp3_sdk.Batch()\n",
    "#for batch in hyp3_sdk.util.chunk(prepared_jobs):\n",
    "#    submitted_jobs += hyp3_isce.submit_prepared_jobs(batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Query all jobs on the server"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobs = hyp3_isce.find_jobs()\n",
    "print(jobs)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:34.044675Z",
     "start_time": "2022-01-26T20:35:33.014286Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Query your particular job"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobs = hyp3_isce.find_jobs(name=job_name)\n",
    "print(jobs)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:34.044675Z",
     "start_time": "2022-01-26T20:35:33.014286Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # create clean directory to deposit products in\n",
    "if os.path.exists(prod_dir):\n",
    "    os.remove(prod_dir)\n",
    "\n",
    "os.mkdir(prod_dir)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:34.044675Z",
     "start_time": "2022-01-26T20:35:33.014286Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we show how to download files. The multi-threading example will download products in parallel much faster than `jobs.download_files()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobs = hyp3_isce.find_jobs(name=job_name)\n",
    "print(jobs)\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(tqdm(executor.map(lambda job: job.download_files(), jobs), total=len(jobs)))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:34.076047Z",
     "start_time": "2022-01-26T20:35:34.046251Z"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5caced1b6f6328ebf252ee0289534035286b1269cd2adef619b938e0a9fdf27"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.4 64-bit ('s1-enumerator': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}