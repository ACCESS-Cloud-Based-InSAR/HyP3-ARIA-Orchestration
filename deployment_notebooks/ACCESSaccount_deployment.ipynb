{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:17:58.574822Z",
     "start_time": "2022-07-28T22:17:58.552683Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is to demonstrate how to use the `s1-enumerator` to get a full time series of GUNWs.\n",
    "\n",
    "We are going basically take each month in acceptable date range and increment by a month and make sure the temporal window is large enough to ensure connectivity across data gaps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters\n",
    "\n",
    "This is what the operator is going to have to change. Will provide some comments."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# toggle user-controlled parameters here\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# product cutline\n",
    "aoi_shapefile = '../aois/CA_pathNumber115.geojson'\n",
    "# load AO geojson and strip variables\n",
    "with open(aoi_shapefile, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    json_keys = data['features'][0]['properties'].keys()\n",
    "    # assign local variables from dict\n",
    "    locals().update(data['features'][0]['properties'])\n",
    "\n",
    "# Override metadata keys\n",
    "# Warning, toggle with caution\n",
    "# Only to be used if you are testing and intentionally updating AO geojsons\n",
    "update_AO = True\n",
    "\n",
    "### Spatial coverage constraint parameter 'azimuth_mismatch'\n",
    "# The merged SLC area over the AOI is allowed to be smaller by 'azimuth_mismatch' x swath width (i.e. 250km)\n",
    "if 'azimuth_mismatch' not in json_keys or update_AO:\n",
    "    azimuth_mismatch = 5 # adjust as necessary\n",
    "    data['features'][0]['properties']['azimuth_mismatch'] = azimuth_mismatch\n",
    "\n",
    "# Specify deployment URL\n",
    "#deploy_url = 'https://hyp3-tibet-jpl.asf.alaska.edu' #for Tibet\n",
    "deploy_url = 'https://hyp3-nisar-jpl.asf.alaska.edu' #for access\n",
    "data['features'][0]['properties']['deploy_url'] = deploy_url\n",
    "\n",
    "# Number of nearest neighbors\n",
    "if 'num_neighbors' not in json_keys or update_AO:\n",
    "    num_neighbors = 2 # adjust as necessary\n",
    "    data['features'][0]['properties']['num_neighbors'] = num_neighbors\n",
    "\n",
    "#set temporal parameters\n",
    "if 'min_days_backward' not in json_keys or update_AO:\n",
    "    min_days_backward = 0\n",
    "    data['features'][0]['properties']['min_days_backward'] = min_days_backward\n",
    "today = datetime.datetime.now()\n",
    "# Earliest year for reference frames\n",
    "START_YEAR = 2014\n",
    "# Latest year for reference frames\n",
    "END_YEAR = today.year\n",
    "YEARS_OF_INTEREST = list(range(START_YEAR,END_YEAR+1))\n",
    "# Adjust depending on seasonality\n",
    "# For annual IFGs, select a single months of interest and you will get what you want.\n",
    "if 'month_range_lower' not in json_keys or update_AO:\n",
    "    month_range_lower = 1 # adjust as necessary\n",
    "    data['features'][0]['properties']['month_range_lower'] = month_range_lower\n",
    "if 'month_range_upper' not in json_keys or update_AO:\n",
    "    month_range_upper = 12 # adjust as necessary\n",
    "    data['features'][0]['properties']['month_range_upper'] = month_range_upper\n",
    "MONTHS_OF_INTEREST = list(range(month_range_lower,month_range_upper+1))\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "## OPTIONAL set temporal sampling parameters\n",
    "\n",
    "# Temporal sampling invervals\n",
    "if 'min_days_backward_timesubset' not in json_keys or update_AO:\n",
    "    # Specify as many temporal sampling intervals as desired (e.g. 90 (days), 180 (days) = semiannual, 365 (days) = annual, etc.)\n",
    "    min_days_backward_timesubset = []\n",
    "    if min_days_backward_timesubset != []:\n",
    "        data['features'][0]['properties']['min_days_backward_timesubset'] = ','.join(map(str, min_days_backward_timesubset))\n",
    "if 'min_days_backward_timesubset' in json_keys:\n",
    "    min_days_backward_timesubset = [int(s) for s in data['features'][0]['properties']['min_days_backward_timesubset'].split(',')]\n",
    "# apply temporal window to all temporal sampling intervals (hardcoded to 60 days)\n",
    "temporal_window_days_timesubset = 60\n",
    "min_days_backward_timesubset = [i - round(temporal_window_days_timesubset/2) for i in min_days_backward_timesubset]\n",
    "if any(x<1 for x in min_days_backward_timesubset):\n",
    "    raise Exception(\"Your specified 'min_days_backward_timesubset' input is too small relative to\"\n",
    "                    \"your specified 'temporal_window_days_timesubset' value. Adjust accordingly\")\n",
    "\n",
    "# Nearest neighbor sampling\n",
    "if 'num_neighbors_timesubset' not in json_keys or update_AO:\n",
    "    # Specify corresponding nearest neighbor sampling for each temporal sampling interval (by default (n-)1)\n",
    "    num_neighbors_timesubset = []\n",
    "    if num_neighbors_timesubset != []:\n",
    "        data['features'][0]['properties']['num_neighbors_timesubset'] = ','.join(map(str, num_neighbors_timesubset))\n",
    "if 'num_neighbors_timesubset' in json_keys:\n",
    "    num_neighbors_timesubset = [int(s) for s in data['features'][0]['properties']['num_neighbors_timesubset'].split(',')]\n",
    "\n",
    "if len (num_neighbors_timesubset) != len(min_days_backward_timesubset):\n",
    "    raise Exception(\"Specified number of temporal sampling intervals DO NOT match specified nearest neighbor sampling\")\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# Define job-name\n",
    "if 'job_name' not in json_keys or update_AO:\n",
    "    job_name = aoi_shapefile.split('/')[-1].split('.')[0].split('pathNumber')\n",
    "    job_name = ''.join(job_name)\n",
    "    job_name = job_name[-20:]\n",
    "    data['features'][0]['properties']['job_name'] = job_name\n",
    "# product directory\n",
    "prod_dir = job_name\n",
    "\n",
    "# if operator variables do not exist, set and populate geojson with them\n",
    "with open(aoi_shapefile, 'w') as file:\n",
    "    json.dump(data, file)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.729834Z",
     "start_time": "2022-01-26T20:33:20.702152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from s1_enumerator import get_aoi_dataframe,  distill_all_pairs, enumerate_ifgs, get_s1_coverage_tiles, enumerate_ifgs_from_stack, get_s1_stack_by_dataframe\n",
    "import concurrent\n",
    "from rasterio.crs import CRS\n",
    "from s1_enumerator import duplicate_gunw_found\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import shape\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import networkx as nx\n",
    "import boto3\n",
    "import hyp3_sdk\n",
    "import copy\n",
    "\n",
    "from deploytools.parse_tools import (shapefile_area, \n",
    "                             continuous_time, \n",
    "                             minimum_overlap_query, \n",
    "                             pair_spatial_check)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.609757Z",
     "start_time": "2022-01-26T20:33:19.458712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_aoi = gpd.read_file(aoi_shapefile)\n",
    "aoi = df_aoi.geometry.unary_union\n",
    "aoi"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.662349Z",
     "start_time": "2022-01-26T20:33:20.614595Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Currently, there is a lot of data in each of the rows above. We really only need the AOI `geometry` and the `path_number`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_numbers = df_aoi.path_number.unique().tolist()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:20.871551Z",
     "start_time": "2022-01-26T20:33:20.834020Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate a stack\n",
    "\n",
    "Using all the tiles that are needed to cover the AOI we make a geometric query based on the frame. We now include only the path we are interested in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_dict = {}\n",
    "path_dict['pathNumber'] = str(path_numbers[0])\n",
    "aoi_geometry = pd.DataFrame([path_dict])\n",
    "aoi_geometry = gpd.GeoDataFrame(aoi_geometry, geometry=[shape(aoi)], crs=CRS.from_epsg(4326))\n",
    "aoi_geometry['pathNumber'] = aoi_geometry['pathNumber'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_stack = get_s1_stack_by_dataframe(aoi_geometry,\n",
    "                                     path_numbers=path_numbers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f'We have {df_stack.shape[0]} frames in our stack'"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.080329Z",
     "start_time": "2022-01-26T20:33:26.048785Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "df_stack.plot(ax=ax, alpha=.5, color='green', label='Frames interesecting tile')\n",
    "df_aoi.exterior.plot(color='black', ax=ax, label='AOI')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.322384Z",
     "start_time": "2022-01-26T20:33:26.081992Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note, we now see the frames cover the entire AOI as we expect."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First remove all scenes that do not produce spatiotemporally contiguous pairs and not meet specified intersection threshold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_stack, df_stack_dict, gap_scenes_dict, rejected_scenes_dict = minimum_overlap_query(df_stack, aoi, azimuth_mismatch=azimuth_mismatch)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.080329Z",
     "start_time": "2022-01-26T20:33:26.048785Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f'We have {df_stack.shape[0]} frames in our stack'"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.080329Z",
     "start_time": "2022-01-26T20:33:26.048785Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot acquisitions that aren't continuous (i.e. have gaps)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not gap_scenes_dict.empty:\n",
    "    gap_scenes_dict =  gap_scenes_dict.sort_values(by=['start_date'])\n",
    "    for index, row in gap_scenes_dict.iterrows():\n",
    "        fig, ax = plt.subplots()\n",
    "        p = gpd.GeoSeries(row['geometry'])\n",
    "        p.exterior.plot(color='black', ax=ax, label=row['start_date_str'][0])\n",
    "        df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "        plt.legend()\n",
    "        plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot all mosaicked acquisitions that were rejected for not meeting user-specified spatial constraints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not rejected_scenes_dict.empty:\n",
    "    rejected_scenes_dict =  rejected_scenes_dict.sort_values(by=['start_date'])\n",
    "    fig, ax = plt.subplots()\n",
    "    for index, row in rejected_scenes_dict.iterrows():\n",
    "        p = gpd.GeoSeries(row['geometry'])\n",
    "        p.exterior.plot(color='black', ax=ax)\n",
    "    df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "    plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot each individual mosaicked acquisitions that were rejected for not meeting user-specified spatial constraints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not rejected_scenes_dict.empty:\n",
    "    for index, row in rejected_scenes_dict.iterrows():\n",
    "        fig, ax = plt.subplots()\n",
    "        p = gpd.GeoSeries(row['geometry'])\n",
    "        p.exterior.plot(color='black', ax=ax, label=row['start_date_str'][0])\n",
    "        df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "        plt.legend()\n",
    "        plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot all mosaicked acquisitions that meet user-defined spatial coverage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for index, row in df_stack_dict.iterrows():\n",
    "    p = gpd.GeoSeries(row['geometry'])\n",
    "    p.exterior.plot(color='black', ax=ax)\n",
    "df_aoi.exterior.plot(color='red', ax=ax, label='AOI')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we filter the stack by month to ensure we only have SLCs we need."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_stack_month = df_stack[df_stack.start_date.dt.month.isin(MONTHS_OF_INTEREST)]\n",
    "df_stack_month = df_stack_month[df_stack_month.start_date.dt.year.isin(YEARS_OF_INTEREST)]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:33:26.356073Z",
     "start_time": "2022-01-26T20:33:26.324043Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will create a list of ```min_reference_dates``` in descending order starting with the most recent date from the SLC stack ```df_stack_month``` as the start date."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_reference_dates = sorted(df_stack_month['startTime'].to_list())\n",
    "min_reference_dates = sorted(list(set([i.replace(hour=0, minute=0, second=0) for i in min_reference_dates])), reverse = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now enumerate the SLC pairs that will produce the interferograms (GUNWs) based on initially defined parameters that are exposed at the top-level of this jupyter notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ifg_pairs = []\n",
    "temporal_window_days = 365*3\n",
    "\n",
    "# Avoid duplicate reference scenes (i.e. extra neighbors than intended)\n",
    "track_ref_dates = []\n",
    "for min_ref_date in tqdm(min_reference_dates):\n",
    "    temp = enumerate_ifgs_from_stack(df_stack_month,\n",
    "                                     aoi,\n",
    "                                     min_ref_date,\n",
    "                                     enumeration_type='tile', # options are 'tile' and 'path'. 'path' processes multiple references simultaneously\n",
    "                                     min_days_backward=min_days_backward,\n",
    "                                     num_neighbors_ref=1,\n",
    "                                     num_neighbors_sec=num_neighbors,\n",
    "                                     temporal_window_days=temporal_window_days,\n",
    "                                     min_tile_aoi_overlap_km2=.1,#Minimum reference tile overlap of AOI in km2\n",
    "                                     min_ref_tile_overlap_perc=.1,#Relative overlap of secondary frames over reference frame\n",
    "                                     minimum_ifg_area_km2=0.1,#The minimum overlap of reference and secondary in km2\n",
    "                                     minimum_path_intersection_km2=.1,#Overlap of common track union with respect to AOI in km2\n",
    "                                         )\n",
    "    if temp != []:\n",
    "        iter_key = temp[0]['reference']['start_date'].keys()[0]\n",
    "        iter_references_scenes = [temp[0]['reference']['start_date'][iter_key]]\n",
    "        if not any(x in iter_references_scenes for x in track_ref_dates):\n",
    "            track_ref_dates.extend(iter_references_scenes)            \n",
    "            ifg_pairs += temp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "OPTIONAL densify network with temporal sampling parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Densify network with specified temporal sampling\n",
    "\n",
    "# Avoid duplicate reference scenes (i.e. extra neighbors than intended)\n",
    "if min_days_backward_timesubset != []:\n",
    "    for t_ind,t_interval in enumerate(min_days_backward_timesubset):\n",
    "        track_ref_dates = []\n",
    "        for min_ref_date in tqdm(min_reference_dates):\n",
    "            temp = enumerate_ifgs_from_stack(df_stack_month,\n",
    "                                             aoi,\n",
    "                                             min_ref_date,\n",
    "                                             enumeration_type='tile', # options are 'tile' and 'path'. 'path' processes multiple references simultaneously\n",
    "                                             min_days_backward=t_interval,\n",
    "                                             num_neighbors_ref=1,\n",
    "                                             num_neighbors_sec=num_neighbors_timesubset[t_ind],\n",
    "                                             temporal_window_days=temporal_window_days_timesubset,\n",
    "                                             min_tile_aoi_overlap_km2=.1,#Minimum reference tile overlap of AOI in km2\n",
    "                                             min_ref_tile_overlap_perc=.1,#Relative overlap of secondary frames over reference frame\n",
    "                                             minimum_ifg_area_km2=0.1,#The minimum overlap of reference and secondary in km2\n",
    "                                             minimum_path_intersection_km2=.1,#Overlap of common track union with respect to AOI in km2\n",
    "                                                 )\n",
    "            if temp != []:\n",
    "                iter_key = temp[0]['reference']['start_date'].keys()[0]\n",
    "                iter_references_scenes = [temp[0]['reference']['start_date'][iter_key]]\n",
    "                if not any(x in iter_references_scenes for x in track_ref_dates):\n",
    "                    track_ref_dates.extend(iter_references_scenes)            \n",
    "                    ifg_pairs += temp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f'The number of GUNWs (likely lots of duplicates) is {len(ifg_pairs)}'"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:46.839854Z",
     "start_time": "2022-01-26T20:34:46.810852Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pairs = distill_all_pairs(ifg_pairs)\n",
    "f\"# of GUNWs: ' {df_pairs.shape[0]}\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:47.146266Z",
     "start_time": "2022-01-26T20:34:47.117927Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deduplication Pt. 1\n",
    "\n",
    "A `GUNW` is uniquely determined by the reference and secondary IDs. We contanenate these sorted lists and generate a lossy hash to deduplicate products we may have introduced from the enumeration above."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T00:51:53.374968Z",
     "start_time": "2022-01-21T00:51:53.344139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "\n",
    "def get_gunw_hash_id(reference_ids: list, secondary_ids: list) -> str:\n",
    "    all_ids = json.dumps([' '.join(sorted(reference_ids)),\n",
    "                          ' '.join(sorted(secondary_ids))\n",
    "                          ]).encode('utf8')\n",
    "    hash_id = hashlib.md5(all_ids).hexdigest()\n",
    "    return hash_id\n",
    "\n",
    "def hasher(row):\n",
    "    return get_gunw_hash_id(row['reference'], row['secondary'])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:47.176882Z",
     "start_time": "2022-01-26T20:34:47.147987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pairs['hash_id'] = df_pairs.apply(hasher, axis=1)\n",
    "f\"# of duplicated entries: {df_pairs.duplicated(subset=['hash_id']).sum()}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pairs = df_pairs.drop_duplicates(subset=['hash_id']).reset_index(drop=True)\n",
    "f\"# of UNIQUE GUNWs: {df_pairs.shape[0]}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Viewing GUNW pairs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# start index\n",
    "M = 0\n",
    "# number of pairs to view\n",
    "N = 5\n",
    "\n",
    "for J in range(M, M + N):\n",
    "    pair = ifg_pairs[J]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, sharey=True, sharex=True)\n",
    "\n",
    "    df_ref_plot = pair['reference']\n",
    "    df_sec_plot = pair['secondary']\n",
    "\n",
    "    df_ref_plot.plot(column='start_date_str', \n",
    "                     legend=True, \n",
    "                     ax=axs[0], alpha=.15)\n",
    "    df_aoi.exterior.plot(ax=axs[0], alpha=.5, color='black')\n",
    "    axs[0].set_title('Reference')\n",
    "\n",
    "    df_sec_plot.plot(column='start_date_str', \n",
    "                     legend=True, \n",
    "                     ax=axs[1], alpha=.15)\n",
    "    df_aoi.exterior.plot(ax=axs[1], alpha=.5, color='black')\n",
    "    \n",
    "    axs[0].set_title(f'Reference {J}')\n",
    "    axs[1].set_title('Secondary')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:48.692684Z",
     "start_time": "2022-01-26T20:34:47.290616Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update types for Graphical Analysis\n",
    "\n",
    "We want to do some basic visualization to support the understanding if we traverse time correctly. We do some simple standard pandas manipulation."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T00:54:06.533180Z",
     "start_time": "2022-01-21T00:54:06.503673Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pairs['reference_date'] = pd.to_datetime(df_pairs['reference_date'])\n",
    "df_pairs['secondary_date'] = pd.to_datetime(df_pairs['secondary_date'])\n",
    "df_pairs.head()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:48.738198Z",
     "start_time": "2022-01-26T20:34:48.698474Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize a Date Graph from Time Series\n",
    "\n",
    "We can put this into a network Directed Graph and use some simple network functions to check connectivity.\n",
    "\n",
    "We are going to use just dates for nodes, though you could use `(ref_date, hash_id)` for nodes and then inspect connected components. That is for another notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get unique dates\n",
    "unique_dates = df_pairs.reference_date.tolist() + df_pairs.secondary_date.tolist()\n",
    "unique_dates = sorted(list(set(unique_dates)))\n",
    "\n",
    "# initiate and plot date notes\n",
    "date2node = {date: k for (k, date) in enumerate(unique_dates)}\n",
    "node2date = {k: date for (date, k) in date2node.items()}\n",
    "\n",
    "%matplotlib widget\n",
    "G = nx.DiGraph()\n",
    "\n",
    "edges = [(date2node[ref_date], date2node[sec_date]) \n",
    "         for (ref_date, sec_date) in zip(df_pairs.reference_date, df_pairs.secondary_date)]\n",
    "G.add_edges_from(edges)\n",
    "nx.draw(G)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function checks there is a path from the first date to the last one. The y-axis is created purely for display so doesn't really indicated anything but flow by month."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nx.has_path(G, \n",
    "            target=date2node[unique_dates[0]],\n",
    "            source=date2node[unique_dates[-1]])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:49.562722Z",
     "start_time": "2022-01-26T20:34:49.532969Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure that the result above returns a ```True``` value to be able to produce a time-series."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "increment = [date.month + date.day for date in unique_dates]\n",
    "\n",
    "# source: https://stackoverflow.com/a/27852570\n",
    "scat = ax.scatter(unique_dates, increment)\n",
    "position = scat.get_offsets().data\n",
    "\n",
    "pos = {date2node[date]: position[k] for (k, date) in enumerate(unique_dates)}\n",
    "nx.draw_networkx_edges(G, pos=pos, ax=ax)\n",
    "ax.grid('on')\n",
    "ax.tick_params(axis='x',\n",
    "               which='major',\n",
    "               labelbottom=True,\n",
    "               labelleft=True)\n",
    "ymin, ymax = ax.get_ylim()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:51.843019Z",
     "start_time": "2022-01-26T20:34:49.564366Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deduplication Pt. 2\n",
    "\n",
    "This is to ensure that previous processing hasn't generate any of the products we have just enumerated.\n",
    "\n",
    "\n",
    "# Check CMR\n",
    "\n",
    "This function checks the ASF DAAC if there are GUNWs with the same spatial extent and same date pairs as the ones created. At some point, we will be able to check the input SLC ids from CMR, but currently that is not possible.\n",
    "\n",
    "If you are processing a new AOI whose products have not been delivered, you can ignore this step. It is a bit time consuming as the queries are done product by product."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import requests\n",
    "\n",
    "COLLECTION_CONCEPT_ID = 'C1595422627-ASF'\n",
    "CMR_URL = 'https://cmr.earthdata.nasa.gov/search/granules.echo10'\n",
    "\n",
    "def parse_echo10(echo10_xml: str):\n",
    "    granules = []\n",
    "    root = ET.fromstring(echo10_xml)\n",
    "    for granule in root.findall('result/Granule'):\n",
    "        g = {\n",
    "            'product_id': granule.find('GranuleUR').text,\n",
    "            'product_version': granule.find('GranuleUR').text.split('-')[-1],\n",
    "            'reference_scenes': [],\n",
    "            'secondary_scenes': []\n",
    "        }\n",
    "        for input_granule in granule.findall('InputGranules/InputGranule'):\n",
    "            input_granule_type, input_granule_name = input_granule.text.split(' ')\n",
    "            if input_granule_type == '[Reference]':\n",
    "                g['reference_scenes'].append(input_granule_name)\n",
    "            else:\n",
    "                g['secondary_scenes'].append(input_granule_name)\n",
    "        granules.append(g)\n",
    "    return granules\n",
    "\n",
    "\n",
    "def get_cmr_products(path: int = None):\n",
    "    session = requests.Session()\n",
    "    search_params = {\n",
    "        'provider': 'ASF',\n",
    "        'collection_concept_id': COLLECTION_CONCEPT_ID,\n",
    "        'page_size': 2000,\n",
    "    }\n",
    "    if path is not None:\n",
    "        search_params['attribute[]'] = f'int,PATH_NUMBER,{path}'\n",
    "    headers = {}\n",
    "    products = []\n",
    "\n",
    "    while True:\n",
    "        response = session.get(CMR_URL, params=search_params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        parsed_results = parse_echo10(response.text)\n",
    "        products.extend(parsed_results)\n",
    "\n",
    "        if 'CMR-Search-After' not in response.headers:\n",
    "            break\n",
    "        headers = {'CMR-Search-After': response.headers['CMR-Search-After']}\n",
    "\n",
    "    return products"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# query CMR for all existing products in path\n",
    "results = []\n",
    "results = get_cmr_products(path_numbers[0])\n",
    "\n",
    "# sort with descending product version numbers\n",
    "results = sorted(results, key=lambda d: d['product_version'], reverse = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert CMR results to dataframe with the latest product version\n",
    "new_results = []\n",
    "track_scenes = []\n",
    "for i in results:\n",
    "    ifg_append = i['reference_scenes'] + i['secondary_scenes']\n",
    "    # pass only first instance of scene combo\n",
    "    if ifg_append not in track_scenes:\n",
    "        track_scenes.append(ifg_append)\n",
    "        new_results.append(i)\n",
    "\n",
    "results = pd.DataFrame(new_results)\n",
    "# update column names for merging\n",
    "results.rename(columns={\"reference_scenes\": \"reference\", \"secondary_scenes\": \"secondary\"}, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Capture products in CMR"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def capture_cmr_products(row, cmr_products):\n",
    "    '''Capture products that exist in CMR based on reference and secondary scenes'''\n",
    "    # concatenate reference and secondary scene in each dataframe row\n",
    "    row_scenes = row['reference'] + row['secondary']\n",
    "    # flag True if scenes from enumerator results are within CMR results\n",
    "    if any(set(row_scenes).issubset(set(x)) for x in cmr_products):\n",
    "        product_on_cmr = True\n",
    "    else:\n",
    "        product_on_cmr = np.nan\n",
    "    return product_on_cmr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Finally filter out pairs in CMR\n",
    "try:\n",
    "    # parse all reference and secondary products for each corresponding product in CMR\n",
    "    cmr_products = results['reference'] + results['secondary']\n",
    "    cmr_products = cmr_products.to_list()\n",
    "    # determine which products in the enumerator already exist in CMR\n",
    "    df_pairs['product_id'] = df_pairs.apply(lambda r: capture_cmr_products(r, cmr_products), axis=1)\n",
    "    # filter out pairs in CMR\n",
    "    total_existing_gunws = len(df_pairs[df_pairs['product_id'].notna()])\n",
    "    print('existing_gunws: ', total_existing_gunws)\n",
    "    print('Total pairs', df_pairs.shape[0])\n",
    "    df_pairs_filtered = df_pairs[~df_pairs['product_id'].notna()].reset_index(drop=True)\n",
    "    df_pairs_filtered.drop_duplicates(subset=['hash_id'], inplace=True)\n",
    "    print('after filtering, total pairs: ', df_pairs_filtered.shape[0])\n",
    "except KeyError:\n",
    "    df_pairs_filtered = copy.deepcopy(df_pairs)\n",
    "    df_pairs_filtered.drop_duplicates(subset=['hash_id'], inplace=True)\n",
    "    print('after filtering, total pairs: ', df_pairs_filtered.shape[0])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:13.765308Z",
     "start_time": "2022-01-26T20:35:13.717055Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if len(df_pairs_filtered) == 0:\n",
    "    raise Exception('All queried pairs are in CMR, there is nothing to process with specified parameters.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check Hyp3 Account\n",
    "\n",
    "We are now going to check\n",
    "\n",
    "1. check products in the open s3 bucket\n",
    "2. check running/pending jobs\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. Above, to accomplish step 1., there is some verbose code (see below). Once we automate delivery, this step will be obsolete. However, until we have delivery, we have to make sure that there are no existing products. Additionally, if we are using a separate (non-operational account), then would be good to use this.\n",
    "2. If we are debugging products and some of our previously generated products were made incorrectly, we will want to ignore this step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# uses .netrc; add `prompt=True` to prompt for credentials; \n",
    "hyp3_isce = hyp3_sdk.HyP3(deploy_url)\n",
    "pending_jobs = hyp3_isce.find_jobs(status_code='PENDING') +  hyp3_isce.find_jobs(status_code='RUNNING')\n",
    "all_jobs = hyp3_isce.find_jobs()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:22.029890Z",
     "start_time": "2022-01-26T20:35:13.809788Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(all_jobs)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:22.059552Z",
     "start_time": "2022-01-26T20:35:22.031459Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Get existing products in s3 bucket"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get bucket (there is only one)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "job_data = [j.to_dict() for j in all_jobs]\n",
    "job_data_s3 = list(filter(lambda job: 'files' in job.keys(), job_data))\n",
    "# Only proceed if there are existing jobs\n",
    "if job_data_s3 != []:\n",
    "    bucket = job_data_s3[0]['files'][0]['s3']['bucket']\n",
    "\n",
    "    # Get all keys\n",
    "    job_keys = [job['files'][0]['s3']['key'] for job in job_data_s3]\n",
    "\n",
    "    s3 = boto3.resource('s3',config=Config(signature_version=UNSIGNED))\n",
    "    prod_bucket = s3.Bucket(bucket)\n",
    "\n",
    "    objects = list(prod_bucket.objects.all())\n",
    "    ncs = list(filter(lambda x: x.key.endswith('.nc'), objects))\n",
    "\n",
    "    # Need to physically check if the products are not there (could have been deleted!)\n",
    "\n",
    "    nc_keys = [nc_ob.key for nc_ob in ncs]\n",
    "    jobs_with_prods_in_s3 = [job for (k, job) in enumerate(job_data_s3) if job_keys[k] in nc_keys]\n",
    "\n",
    "    slcs = [(job['job_parameters']['granules'],\n",
    "            job['job_parameters']['secondary_granules']) \n",
    "            for job in jobs_with_prods_in_s3]\n",
    "\n",
    "    hash_ids_of_prods_in_s3 = [get_gunw_hash_id(*slc) for slc in slcs]\n",
    "\n",
    "    f\"We are removing {df_pairs_filtered['hash_id'].isin(hash_ids_of_prods_in_s3).sum()} GUNWs for submission\"\n",
    "\n",
    "    items = hash_ids_of_prods_in_s3\n",
    "    df_pairs_filtered = df_pairs_filtered[~df_pairs_filtered['hash_id'].isin(items)].reset_index(drop=True)\n",
    "    f\"Current # of GUNWs: {df_pairs_filtered.shape[0]}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Running or Pending Jobs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Only proceed if there are existing jobs\n",
    "if job_data_s3 != []:\n",
    "    pending_job_data = [j.to_dict() for j in pending_jobs]\n",
    "    pending_slcs = [(job['job_parameters']['granules'],\n",
    "                    job['job_parameters']['secondary_granules']) \n",
    "                    for job in pending_job_data]\n",
    "\n",
    "    hash_ids_of_pending_jobs = [get_gunw_hash_id(*slc) for slc in pending_slcs]\n",
    "\n",
    "    items = hash_ids_of_pending_jobs\n",
    "    f\"We are removing {df_pairs_filtered['hash_id'].isin(items).sum()} GUNWs for submission\"\n",
    "\n",
    "    items = hash_ids_of_pending_jobs\n",
    "    df_pairs_filtered = df_pairs_filtered[~df_pairs_filtered['hash_id'].isin(items)].reset_index(drop=True)\n",
    "    f\"Current # of GUNWs: {df_pairs_filtered.shape[0]}\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.237660Z",
     "start_time": "2022-01-26T20:35:27.209164Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize a Date Graph from the Final Filtered Time Series\n",
    "\n",
    "We can put this into a network Directed Graph and use some simple network functions to check connectivity (*which may not be applicable).\n",
    "\n",
    "We are going to use just dates for nodes, though you could use `(ref_date, hash_id)` for nodes and then inspect connected components. That is for another notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get unique dates\n",
    "unique_dates = df_pairs_filtered.reference_date.tolist() + df_pairs_filtered.secondary_date.tolist()\n",
    "unique_dates = sorted(list(set(unique_dates)))\n",
    "\n",
    "# initiate and plot date notes\n",
    "date2node = {date: k for (k, date) in enumerate(unique_dates)}\n",
    "node2date = {k: date for (date, k) in date2node.items()}\n",
    "\n",
    "%matplotlib widget\n",
    "G = nx.DiGraph()\n",
    "\n",
    "edges = [(date2node[ref_date], date2node[sec_date]) \n",
    "         for (ref_date, sec_date) in zip(df_pairs_filtered.reference_date, df_pairs_filtered.secondary_date)]\n",
    "G.add_edges_from(edges)\n",
    "nx.draw(G)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function checks there is a path from the first date to the last one. The y-axis is created purely for display so doesn't really indicated anything but flow by month."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Again, this may not be applicable in cases where parts of the network had already been deployed before and/or you are densifying by specifying temporal sampling.\n",
    "In such cases, these plots serve merely as a sanity check."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nx.has_path(G, \n",
    "            target=date2node[unique_dates[0]],\n",
    "            source=date2node[unique_dates[-1]])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:49.562722Z",
     "start_time": "2022-01-26T20:34:49.532969Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure that the result above returns a ```True``` value to be able to produce a time-series."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "increment = [date.month + date.day for date in unique_dates]\n",
    "\n",
    "# source: https://stackoverflow.com/a/27852570\n",
    "scat = ax.scatter(unique_dates, increment)\n",
    "position = scat.get_offsets().data\n",
    "\n",
    "pos = {date2node[date]: position[k] for (k, date) in enumerate(unique_dates)}\n",
    "nx.draw_networkx_edges(G, pos=pos, ax=ax)\n",
    "ax.grid('on')\n",
    "ax.tick_params(axis='x',\n",
    "               which='major',\n",
    "               labelbottom=True,\n",
    "               labelleft=True)\n",
    "ymin, ymax = ax.get_ylim()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:34:51.843019Z",
     "start_time": "2022-01-26T20:34:49.564366Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submit jobs to Hyp3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "records_to_submit = df_pairs_filtered.to_dict('records')\n",
    "records_to_submit[0]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:27.366597Z",
     "start_time": "2022-01-26T20:35:27.335100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The below puts the records in a format that we can submit to the Hyp3 API.\n",
    "\n",
    "**Note 1**: there is an index in the records to submit to ensure we don't over submit jobs for generating GUNWs. \\\n",
    "**Note 2**: uncomment the code to *actually* submit the jobs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# uses .netrc; add `prompt=True` to prompt for credentials; \n",
    "hyp3_isce = hyp3_sdk.HyP3(deploy_url)\n",
    "\n",
    "# NOTE: we are using \"INSAR_ISCE\" for the `main` branch.\n",
    "# Change this to \"INSAR_ISCE_TEST\" to use the `dev` branch, but ONLY if you know what you're doing\n",
    "# chaging to dev will overwrite the product version number and make it difficult to dedup\n",
    "job_type = 'INSAR_ISCE'\n",
    "\n",
    "job_dicts = [{'name': job_name,\n",
    "              'job_type': job_type,\n",
    "              'job_parameters': {'granules': r['reference'],\n",
    "                                 'secondary_granules': r['secondary']}} \n",
    "             # NOTE THERE IS AN INDEX - this is to submit only a subset of Jobs\n",
    "             for r in records_to_submit]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:33.012813Z",
     "start_time": "2022-01-26T20:35:30.285294Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Report summary of all job parameters\n",
    "print(\"Start date is '{}'\".format(unique_dates[0]))\n",
    "print(\"End date is '{}'\".format(unique_dates[-1]))\n",
    "print(\"GUNWs expected '{}'\".format(len(job_dicts)))\n",
    "print(\"Job Name is '{}'\".format(job_name))\n",
    "print(\"Shapefile Name is '{}'\".format(aoi_shapefile.split('/')[-1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#UNCOMMENT TO SUBMIT\n",
    "#prepared_jobs = job_dicts\n",
    "#submitted_jobs = hyp3_sdk.Batch()\n",
    "#for batch in hyp3_sdk.util.chunk(prepared_jobs):\n",
    "#    submitted_jobs += hyp3_isce.submit_prepared_jobs(batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Query all jobs on the server"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobs = hyp3_isce.find_jobs()\n",
    "print(jobs)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:34.044675Z",
     "start_time": "2022-01-26T20:35:33.014286Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Query your particular job"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobs = hyp3_isce.find_jobs(name=job_name)\n",
    "print(jobs)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:34.044675Z",
     "start_time": "2022-01-26T20:35:33.014286Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # create clean directory to deposit products in\n",
    "if os.path.exists(prod_dir):\n",
    "    os.remove(prod_dir)\n",
    "\n",
    "os.mkdir(prod_dir)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:34.044675Z",
     "start_time": "2022-01-26T20:35:33.014286Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we show how to download files. The multi-threading example will download products in parallel much faster than `jobs.download_files()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobs = hyp3_isce.find_jobs(name=job_name)\n",
    "print(jobs)\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(tqdm(executor.map(lambda job: job.download_files(), jobs), total=len(jobs)))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T20:35:34.076047Z",
     "start_time": "2022-01-26T20:35:34.046251Z"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5caced1b6f6328ebf252ee0289534035286b1269cd2adef619b938e0a9fdf27"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.4 64-bit ('s1-enumerator': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}